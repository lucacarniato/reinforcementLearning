{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREDIT: https://pythonprogramming.net/custom-environment-reinforcement-learning-stable-baselines-3-tutorial/?completed=/saving-and-loading-reinforcement-learning-stable-baselines-3-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The snake game costumized enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collision_with_apple(apple_position):\n",
    "    apple_position = [random.randrange(1,50)*10,random.randrange(1,50)*10]\n",
    "    return apple_position\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if snake_head[0]>=500 or snake_head[0]<0 or snake_head[1]>=500 or snake_head[1]<0 :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "class SnakeEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(SnakeEnv, self).__init__()\n",
    "        \n",
    "        self.snake_obs = 10 * 2\n",
    "        self.snake_initial_length = 3\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=-500, high=500, shape=(5+self.snake_obs,), dtype=np.float32)\n",
    "        # however long we aspire the snake to be\n",
    "        self.prev_actions = deque(maxlen = self.snake_obs)  \n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Change the head position based on the button direction\n",
    "        if action == 1:\n",
    "            self.snake_head[0] += 10\n",
    "        elif action == 0:\n",
    "            self.snake_head[0] -= 10\n",
    "        elif action == 2:\n",
    "            self.snake_head[1] += 10\n",
    "        elif action == 3:\n",
    "            self.snake_head[1] -= 10\n",
    "\n",
    "        # Increase Snake length on eating apple\n",
    "        apple_reward = 0.0\n",
    "        if self.snake_head == self.apple_position:\n",
    "            self.apple_position = collision_with_apple(self.apple_position)\n",
    "            self.snake_position.insert(0,list(self.snake_head))\n",
    "            apple_reward = 2.0 *(len(self.snake_position)-3)\n",
    "        else:\n",
    "            self.snake_position.insert(0,list(self.snake_head))\n",
    "            self.snake_position.pop()\n",
    "        \n",
    "        # On collision kill the snake\n",
    "        if collision_with_boundaries(self.snake_head) == 1 or collision_with_self(self.snake_position) == 1:\n",
    "            self.done = True\n",
    "            self.reward = -2.0\n",
    "        else:\n",
    "            euclidean_dist_to_apple = np.linalg.norm(np.array(self.snake_head) - np.array(self.apple_position))\n",
    "            self.total_reward = (self.initial_distance - euclidean_dist_to_apple)/self.initial_distance + apple_reward \n",
    "            self.reward = self.total_reward - self.prev_total_reward\n",
    "            self.prev_total_reward = self.total_reward\n",
    "        \n",
    "        observation = self._compute_observation()\n",
    "        \n",
    "        return observation, self.reward, self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250,250],[240,250],[230,250]]\n",
    "        self.apple_position = [random.randrange(1,50)*10,random.randrange(1,50)*10]\n",
    "        self.snake_head = [250,250]\n",
    "        self.initial_distance = np.linalg.norm(np.array(self.snake_head) - np.array(self.apple_position)) + 1.0e-6\n",
    "        \n",
    "        self.prev_total_reward = 0.0\n",
    "        \n",
    "        # empty actions\n",
    "        for i in range(self.snake_obs):\n",
    "            self.prev_actions.append(-1) \n",
    "            \n",
    "        self.done = False\n",
    "        observation = self._compute_observation()\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \n",
    "        # Create image\n",
    "        self.img = np.zeros((500,500,3),dtype='uint8')        \n",
    "        # Display Apple\n",
    "        cv2.rectangle(self.img,(self.apple_position[0],self.apple_position[1]),(self.apple_position[0]+10,self.apple_position[1]+10),(0,0,255),3)\n",
    "        \n",
    "        # Display Snake\n",
    "        for position in self.snake_position:\n",
    "            cv2.rectangle(self.img,(position[0],position[1]),(position[0]+10,position[1]+10),(0,255,0),3)\n",
    "            \n",
    "        # Display collision text\n",
    "        if collision_with_boundaries(self.snake_head) == 1 or collision_with_self(self.snake_position) == 1:\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            self.img = np.zeros((500,500,3),dtype='uint8')\n",
    "            cv2.putText(self.img,'Snake length {}'.format(len(self.snake_position)),(140,250), font, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('a',self.img)\n",
    "        cv2.waitKey(1)\n",
    "    \n",
    "    def _compute_observation(self):\n",
    "        head_x = self.snake_head[0]\n",
    "        head_y = self.snake_head[1]\n",
    "        snake_length = len(self.snake_position)\n",
    "        apple_delta_x = self.apple_position[0] - head_x\n",
    "        apple_delta_y = self.apple_position[1] - head_y\n",
    "        \n",
    "        snake_len = len(self.snake_position)\n",
    "        for i in range(1,len(self.snake_position)):\n",
    "            self.prev_actions.append(self.snake_position[i-1][0] -self.snake_position[i][0]) \n",
    "            self.prev_actions.append(self.snake_position[i-1][1] -self.snake_position[i][1]) \n",
    "        \n",
    "        observation = [head_x, head_y, apple_delta_x, apple_delta_y, snake_length] + list(self.prev_actions)\n",
    "        observation = np.array(observation)\n",
    "        \n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv()\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        random_action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(random_action)\n",
    "        #env.render()\n",
    "        print('reward',reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a stable baselines 3 algorithm to compute the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\apps\\python_3_8_6\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"PPO\"\n",
    "models_dir = \"models/\" + model_name\n",
    "logdir = \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models at different iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "if model_name ==\"PPO\":\n",
    "    model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=logdir)\n",
    "elif model_name ==\"A2C\":\n",
    "    model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs\\PPO_0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.83     |\n",
      "|    ep_rew_mean     | -2.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 887      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.4         |\n",
      "|    ep_rew_mean          | -1.98       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 986         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015775353 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.559      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0595      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.389       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.43        |\n",
      "|    ep_rew_mean          | -1.97       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1018        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015758662 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0537     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    value_loss           | 1.1e+12     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.4         |\n",
      "|    ep_rew_mean          | -1.91       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1032        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020269498 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.0709      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.21        |\n",
      "|    ep_rew_mean          | -1.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1048        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020002423 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | -0.0223     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0513     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    value_loss           | 0.0715      |\n",
      "-----------------------------------------\n",
      "Logging to logs\\PPO_0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.93     |\n",
      "|    ep_rew_mean     | -1.86    |\n",
      "| time/              |          |\n",
      "|    fps             | 2211     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.4        |\n",
      "|    ep_rew_mean          | -1.69       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1442        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021642447 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00615    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    value_loss           | 0.275       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 16.1        |\n",
      "|    ep_rew_mean          | -1.56       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1303        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013440798 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.881      |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00232     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "for i in range(20):\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=model_name)\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "model_path = f\"{models_dir}/200000.zip\"\n",
    "\n",
    "if model_name ==\"PPO\":\n",
    "    model = PPO.load(model_path, env=env)\n",
    "elif model_name ==\"A2C\":\n",
    "    model = A2C.load(model_path, env=env)\n",
    "\n",
    "episodes = 50\n",
    "for ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # pass observation to model to get predicted action\n",
    "        action, _states = model.predict(obs)\n",
    "        # pass action to env and get info back\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        \n",
    "        # show the environment on the screen\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
